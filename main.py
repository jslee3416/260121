import streamlit as st
import pandas as pd
import urllib.parse
import os

# 1. í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ë° ì œëª© ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ êµ¬ê¸€ í‰ì  íŒŒì¸ë”", layout="wide")

@st.cache_data
def load_and_clean_data(file_name):
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(current_dir, file_name)
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(file_path):
            return pd.DataFrame()
        
        # ì¸ì½”ë”© ì²˜ë¦¬ (CP949, UTF-8 ìˆœì°¨ ì‹œë„)
        df = None
        for enc in ['utf-8', 'cp949', 'euc-kr']:
            try:
                df = pd.read_csv(file_path, encoding=enc, sep=None, engine='python')
                if df is not None and not df.empty:
                    break
            except:
                continue
        
        if df is None or df.empty:
            return pd.DataFrame()

        # [ì»¬ëŸ¼ ë§¤ì¹­] ì„œìš¸ê´€ê´‘ì¬ë‹¨ ë°ì´í„° ê¸°ì¤€
        name_map = {
            'ì‹ë‹¹ëª…': 'ìƒí˜¸',
            'ì§€ì—­ëª…': 'ì§€ì—­',
            'ëŒ€í‘œë©”ë‰´ëª…': 'ëŒ€í‘œë©”ë‰´'
        }
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ì´ë¦„ ë³€ê²½
        existing_cols = [c for c in name_map.keys() if c in df.columns]
        df = df[existing_cols].rename(columns=name_map)
        
        # [í–‰ì •êµ¬ì—­ ë¶„ë¦¬] 'ì§€ì—­' ì»¬ëŸ¼ì—ì„œ êµ¬ì™€ ë™ ì¶”ì¶œ
        # ì˜ˆ: "ê°•ë‚¨êµ¬ ì—­ì‚¼ë™" -> êµ¬: ê°•ë‚¨êµ¬, ë™: ì—­ì‚¼ë™
        def split_region(x):
            if pd.isna(x): return "ë¯¸ë¶„ë¥˜", "ë¯¸ë¶„ë¥˜"
            parts = str(x).split()
            gu = parts[0] if len(parts) > 0 else "ë¯¸ë¶„ë¥˜"
            dong = parts[1] if len(parts) > 1 else "ì „ì²´"
            return gu, dong

        df[['êµ¬', 'ë™']] = df['ì§€ì—­'].apply(lambda x: pd.Series(split_region(x)))
        
        return df.reset_index(drop=True)
        
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# ìš”ì²­í•˜ì‹  ëŒ€ë¡œ ë°ì´í„° íŒŒì¼ëª… ìˆ˜ì •
DATA_FILE = "restaurants.csv"
df = load_and_clean_data(DATA_FILE)

# 2. ë©”ì¸ UI êµ¬ì„±
st.title("ğŸ´ ì„œìš¸ ë§›ì§‘ ì‹¤ì‹œê°„ í‰ì  íƒìƒ‰ê¸°")
st.markdown("---")

if not df.empty:
    # --- ì‚¬ì´ë“œë°”: í–‰ì •êµ¬ì—­ í•„í„° ---
    st.sidebar.header("ğŸ“ ì§€ì—­ í•„í„°")
    
    # êµ¬ ì„ íƒ
    gu_list = sorted(df['êµ¬'].unique())
    selected_gu = st.sidebar.selectbox("ìì¹˜êµ¬ ì„ íƒ", gu_list)
    
    # ì„ íƒëœ êµ¬ì— í•´ë‹¹í•˜ëŠ” ë™ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    dong_options = sorted(df[df['êµ¬'] == selected_gu]['ë™'].unique())
    selected_dong = st.sidebar.selectbox("ë²•ì •ë™ ì„ íƒ", ["ì „ì²´"] + dong_options)
    
    # í•„í„°ë§ ë¡œì§
    if selected_dong == "ì „ì²´":
        filtered_df = df[df['êµ¬'] == selected_gu]
    else:
        filtered_df = df[(df['êµ¬'] == selected_gu) & (df['ë™'] == selected_dong)]

    # í‚¤ì›Œë“œ ê²€ìƒ‰
    search_query = st.sidebar.text_input("ğŸ” ì‹ë‹¹ ì´ë¦„ ê²€ìƒ‰", "")
    if search_query:
        filtered_df = filtered_df[filtered_df['ìƒí˜¸'].str.contains(search_query, na=False)]

    # --- ë©”ì¸ ê²°ê³¼ ì¶œë ¥ ---
    st.subheader(f"ğŸ“ {selected_gu} {selected_dong if selected_dong != 'ì „ì²´' else ''} ë§›ì§‘ ëª©ë¡")
    st.info(f"ì„ íƒ ì§€ì—­ ì‹ë‹¹: **{len(filtered_df)}ê°œ** (ì‹ë‹¹ëª…ì„ í´ë¦­í•˜ë©´ êµ¬ê¸€ ë§µ í‰ì ìœ¼ë¡œ ì—°ê²°ë©ë‹ˆë‹¤)")

    if not filtered_df.empty:
        # í˜ì´ì§€ë„¤ì´ì…˜ (15ê°œì”©)
        rows_per_page = 15
        total_pages = max(len(filtered_df) // rows_per_page + (1 if len(filtered_df) % rows_per_page > 0 else 0), 1)
        
        col_page, _ = st.columns([1, 4])
        with col_page:
            current_page = st.number_input(f"í˜ì´ì§€ (1/{total_pages})", 1, total_pages, 1)
        
        start_idx = (current_page - 1) * rows_per_page
        page_data = filtered_df.iloc[start_idx : start_idx + rows_per_page].copy()

        # êµ¬ê¸€ ë§µ ê²€ìƒ‰ ë§í¬ ìƒì„± í•¨ìˆ˜
        def make_google_link(row):
            # ì •í™•í•œ ê²€ìƒ‰ì„ ìœ„í•´ êµ¬+ë™+ìƒí˜¸ ì¡°í•©
            query = urllib.parse.quote(f"{row['êµ¬']} {row['ë™']} {row['ìƒí˜¸']}")
            return f"https://www.google.com/maps/search/{query}"

        # í‘œ ì¶œë ¥
        st.markdown("---")
        header = "| ë²ˆí˜¸ | ì‹ë‹¹ëª… | ëŒ€í‘œë©”ë‰´ | êµ¬ê¸€ ë§µ ì‹¤ì‹œê°„ í‰ì  |"
        sep = "| :--- | :--- | :--- | :--- |"
        rows = []
        
        for i, (_, row) in enumerate(page_data.iterrows()):
            menu = row['ëŒ€í‘œë©”ë‰´'] if pd.notna(row['ëŒ€í‘œë©”ë‰´']) else "-"
            google_url = make_google_link(row)
            link_text = f"[â­ í‰ì /ë¦¬ë·° í™•ì¸í•˜ê¸°]({google_url})"
            rows.append(f"| {start_idx + i + 1} | **{row['ìƒí˜¸']}** | {menu} | {link_text} |")

        st.markdown(header + "\n" + sep + "\n" + "\n".join(rows))
    else:
        st.warning("ì„ íƒí•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.error(f"'{DATA_FILE}' íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.markdown("""
    ### ğŸ› ï¸ í•´ê²° ë°©ë²•
    1. GitHub ì €ì¥ì†Œì— íŒŒì¼ ì´ë¦„ì´ **`restaurants.csv`**ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
    2. íŒŒì¼ ë‚´ìš©ì— 'ì‹ë‹¹ëª…', 'ì§€ì—­ëª…' ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    """)
