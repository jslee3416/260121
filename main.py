import streamlit as st
import pandas as pd
import urllib.parse
import os

st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ íŒŒì¸ë”", layout="wide")

DATA_FILE = "restaurants.csv"

@st.cache_data
def load_and_clean_data(file_name):
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(current_dir, file_name)
        
        if not os.path.exists(file_path):
            st.error(f"íŒŒì¼ ì—†ìŒ: {file_name}")
            return pd.DataFrame()

        # 1. ì¸ì½”ë”© ë° êµ¬ë¶„ì ìë™ ê°ì§€ ë¡œì§
        df = None
        for enc in ['cp949', 'utf-8', 'euc-kr', 'utf-8-sig']:
            try:
                # sep=None, engine='python'ì€ ì‰¼í‘œ/íƒ­/ì„¸ë¯¸ì½œë¡  ìë™ ê°ì§€
                df = pd.read_csv(file_path, encoding=enc, sep=None, engine='python')
                if df is not None and not df.empty:
                    # ë¹ˆ ì¹¸(Unnamed) ì»¬ëŸ¼ ì œê±° ë° ì •ë¦¬
                    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
                    break
            except:
                continue
        
        if df is None or df.empty:
            return pd.DataFrame()

        # [ì§„ë‹¨ìš©] ì‹¤ì œ íŒŒì¼ì˜ ì»¬ëŸ¼ëª…ì„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤Œ
        actual_columns = df.columns.tolist()
        st.sidebar.info(f"ğŸ” íŒŒì¼ ë‚´ ì‹¤ì œ ì»¬ëŸ¼ëª…: {actual_columns}")

        # 2. ìœ ì—°í•œ ì»¬ëŸ¼ ë§¤ì¹­ (ì´ë¦„ì´ ì¡°ê¸ˆ ë‹¬ë¼ë„ ì°¾ì•„ëƒ„)
        # ìƒí˜¸ëª… í›„ë³´: ì‹ë‹¹ëª…, ìƒí˜¸ëª…, ìƒí˜¸, ì—…ì†Œëª…, ì‹ë‹¹(ID)
        # ì§€ì—­ëª… í›„ë³´: ì§€ì—­ëª…, ì§€ì—­, ì£¼ì†Œ, ì†Œì¬ì§€, ìì¹˜êµ¬ëª…
        name_map = {}
        cols = df.columns
        
        name_map['ìƒí˜¸'] = next((c for c in cols if c in ['ì‹ë‹¹ëª…', 'ìƒí˜¸ëª…', 'ìƒí˜¸', 'ì—…ì†Œëª…', 'ì‹ë‹¹(ID)']), None)
        name_map['ì§€ì—­'] = next((c for c in cols if c in ['ì§€ì—­ëª…', 'ì§€ì—­', 'ì£¼ì†Œ', 'ì†Œì¬ì§€', 'ìì¹˜êµ¬ëª…', 'ì§€ì—­ëª… ']), None)
        name_map['ë©”ë‰´'] = next((c for c in cols if c in ['ëŒ€í‘œë©”ë‰´ëª…', 'ëŒ€í‘œë©”ë‰´', 'ë©”ë‰´', 'ì£¼ìš”ë©”ë‰´']), None)

        # í•„ìˆ˜ ì»¬ëŸ¼(ìƒí˜¸, ì§€ì—­)ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì„ ê°•ì œë¡œ í• ë‹¹
        if not name_map['ìƒí˜¸']: name_map['ìƒí˜¸'] = cols[0]
        if not name_map['ì§€ì—­']: name_map['ì§€ì—­'] = cols[1] if len(cols) > 1 else cols[0]

        # ë°ì´í„° ì¬êµ¬ì„±
        final_cols = [v for v in name_map.values() if v is not None]
        inv_map = {v: k for k, v in name_map.items() if v is not None}
        
        df = df[final_cols].rename(columns=inv_map)
        
        # 3. í–‰ì •êµ¬ì—­ ë¶„ë¦¬
        def split_region(x):
            if pd.isna(x): return "ë¯¸ë¶„ë¥˜", "ë¯¸ë¶„ë¥˜"
            parts = str(x).strip().split()
            gu = parts[0] if len(parts) > 0 else "ë¯¸ë¶„ë¥˜"
            dong = " ".join(parts[1:]) if len(parts) > 1 else "ì „ì²´"
            return gu, dong

        df[['êµ¬', 'ë™']] = df['ì§€ì—­'].apply(lambda x: pd.Series(split_region(x)))
        
        return df.reset_index(drop=True)
        
    except Exception as e:
        st.error(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

df = load_and_clean_data(DATA_FILE)

# 2. UI êµ¬ì„±
st.title("ğŸ´ ì„œìš¸ ë§›ì§‘ ì‹¤ì‹œê°„ í‰ì  ê°€ì´ë“œ")

if not df.empty:
    st.sidebar.success("âœ… ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
    
    # ì§€ì—­ í•„í„°
    gu_list = sorted(df['êµ¬'].unique())
    selected_gu = st.sidebar.selectbox("ìì¹˜êµ¬ ì„ íƒ", gu_list)
    
    dong_options = sorted(df[df['êµ¬'] == selected_gu]['ë™'].unique())
    selected_dong = st.sidebar.selectbox("ë²•ì •ë™ ì„ íƒ", ["ì „ì²´"] + dong_options)
    
    filtered_df = df[df['êµ¬'] == selected_gu]
    if selected_dong != "ì „ì²´":
        filtered_df = filtered_df[filtered_df['ë™'] == selected_dong]

    st.subheader(f"ğŸ“ {selected_gu} {selected_dong if selected_dong != 'ì „ì²´' else ''} ë§›ì§‘ ëª©ë¡")

    if not filtered_df.empty:
        # í˜ì´ì§€ë„¤ì´ì…˜
        rows_per_page = 15
        total_pages = max(len(filtered_df) // rows_per_page + (1 if len(filtered_df) % rows_per_page > 0 else 0), 1)
        current_page = st.number_input(f"í˜ì´ì§€ (1/{total_pages})", 1, total_pages, 1)
        
        start_idx = (current_page - 1) * rows_per_page
        page_data = filtered_df.iloc[start_idx : start_idx + rows_per_page].copy()

        # í…Œì´ë¸” ì¶œë ¥
        st.markdown("---")
        st.markdown("| ë²ˆí˜¸ | ì‹ë‹¹ëª… | ì§€ì—­(êµ¬/ë™) | ì‹¤ì‹œê°„ êµ¬ê¸€ í‰ì  ë§í¬ |")
        st.markdown("| :--- | :--- | :--- | :--- |")
        
        for i, (_, row) in enumerate(page_data.iterrows()):
            query = urllib.parse.quote(f"{row['êµ¬']} {row['ë™']} {row['ìƒí˜¸']}")
            google_url = f"https://www.google.com/maps/search/{query}"
            st.markdown(f"| {start_idx + i + 1} | **{row['ìƒí˜¸']}** | {row['êµ¬']} {row['ë™']} | [â­ í‰ì  í™•ì¸í•˜ê¸°]({google_url}) |")
    else:
        st.warning("ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.error("ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.info("ì‚¬ì´ë“œë°”ì— í‘œì‹œëœ 'íŒŒì¼ ë‚´ ì‹¤ì œ ì»¬ëŸ¼ëª…'ì„ í™•ì¸í•´ ë³´ì„¸ìš”.")
