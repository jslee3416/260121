import streamlit as st
import pandas as pd
import requests
import io
import urllib.parse

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ TOP 20", layout="wide")

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ ID
GOOGLE_FILE_ID = '15qLFBk-cWaGgGxe2sPz_FdgeYpquhQa4'

@st.cache_data(show_spinner=False)
def load_data_robust(file_id):
    URL = f"https://docs.google.com/uc?export=download&id={file_id}"
    session = requests.Session()
    
    try:
        # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ 'ë°”ì´ëŸ¬ìŠ¤ ê²€ì‚¬ ë¶ˆê°€' ê²½ê³ ê°€ ëœ° ìˆ˜ ìˆì–´ 2ë²ˆ ì‹œë„í•©ë‹ˆë‹¤.
        response = session.get(URL, stream=True, timeout=60)
        
        # ì¸ì½”ë”© í›„ë³´êµ° (í•œêµ­ ê³µê³µë°ì´í„°ëŠ” ëŒ€ë¶€ë¶„ ì´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤)
        # cp949(ìœˆë„ìš° í•œê¸€), utf-8-sig(BOM í¬í•¨ UTF8), euc-kr(í™•ì¥ í•œê¸€)
        for enc in ['cp949', 'utf-8-sig', 'euc-kr']:
            try:
                # [ì¤‘ìš”] í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì§€ì •í•˜ê³  ë°ì´í„° íƒ€ì…ì„ ë¬¸ìì—´(str)ë¡œ ê°•ì œí•˜ì—¬ íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€
                # 4ë²ˆì§¸(3:ìƒíƒœ), 9ë²ˆì§¸(8:ì´ë¦„), 10ë²ˆì§¸(9:ì—…ì¢…), 19ë²ˆì§¸(18:ì£¼ì†Œ)
                df = pd.read_csv(
                    io.BytesIO(response.content),
                    encoding=enc,
                    usecols=[3, 8, 9, 18],
                    on_bad_lines='skip',  # ê¹¨ì§„ í–‰ ë¬´ì‹œ
                    low_memory=False,     # ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì•ˆì •ì„±
                    dtype=str             # ëª¨ë“  ì—´ì„ ì¼ë‹¨ í…ìŠ¤íŠ¸ë¡œ ì½ìŒ
                )
                
                # ì»¬ëŸ¼ ì´ë¦„ ì¬ì •ì˜
                df.columns = ['status', 'name', 'category', 'address']
                
                # [ìš”êµ¬ì‚¬í•­] 'íì—…' ë°ì´í„° ì‚­ì œ
                # ê²°ì¸¡ì¹˜ë¥¼ ì œê±°í•˜ê³  'íì—…' ê¸€ìê°€ ì—†ëŠ” í–‰ë§Œ í•„í„°ë§
                df = df[~df['status'].fillna('').str.contains("íì—…|ì·¨ì†Œ|ë§ì†Œ")].copy()
                
                # ë°ì´í„°ê°€ ì •ìƒì ìœ¼ë¡œ ì½í˜”ë‹¤ë©´ ë°˜ë³µë¬¸ ì¢…ë£Œ
                if not df.empty:
                    return df
            except Exception:
                continue
                
        return "ë°ì´í„°ì˜ ì¸ì½”ë”©ì„ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (UTF-8/CP949 ëª¨ë‘ ì‹¤íŒ¨)"
        
    except Exception as e:
        return f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}"

# --- ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ---
st.title("ğŸ´ ì„œìš¸ì‹œ ì‹¤ì‹œê°„ ë§›ì§‘ ì¶”ì²œ ê°€ì´ë“œ")

with st.spinner('ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...'):
    data = load_data_robust(GOOGLE_FILE_ID)

if isinstance(data, str):
    st.error(data)
    st.markdown("âš ï¸ **ê³µìœ  ê¶Œí•œì´ ë§ëŠ”ë°ë„ ì•ˆ ëœë‹¤ë©´?**")
    st.write("1. íŒŒì¼ì´ .csv ì¸ì§€ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”. (.xlsxë¼ë©´ ì½”ë“œê°€ ë‹¤ë¦…ë‹ˆë‹¤)")
    st.write("2. êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ 'ë‹¤ìš´ë¡œë“œ'ê°€ ê¸ˆì§€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    st.success(f"âœ… {len(data):,}ê°œì˜ ì‹ë‹¹ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    # ì¹´í…Œê³ ë¦¬ LoV ìƒì„±
    category_list = sorted(data['category'].dropna().unique().tolist())
    selected_category = st.selectbox("ğŸ± ìŒì‹ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì „ì²´"] + category_list)

    filtered_df = data if selected_category == "ì „ì²´" else data[data['category'] == selected_category]

    st.divider()
    st.subheader(f"ğŸ“ '{selected_category}' ì¶”ì²œ ë§›ì§‘ TOP 20")

    # ìƒìœ„ 20ê°œ ì¶œë ¥ ë° êµ¬ê¸€ë§µ ì—°ë™
    for i, row in filtered_df.head(20).iterrows():
        # ê²€ìƒ‰ ì¿¼ë¦¬: ì‹ë‹¹ëª… + ì—…ì¢… + í‰ì /ë¦¬ë·°
        search_q = f"ì„œìš¸ {row['name']} {row['category']} í‰ì  ë¦¬ë·°"
        map_q = f"{row['name']} {row['address']}"
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"### {row['name']}")
            st.write(f"ğŸ“‚ {row['category']} | ğŸ“ {row['address']}")
        with col2:
            st.write("") # ê°„ê²© ì¡°ì ˆ
            st.markdown(f"[â­ í‰ì  í™•ì¸](https://www.google.com/search?q={urllib.parse.quote(search_q)})")
            st.markdown(f"[ğŸ“ ìƒì„¸ ìœ„ì¹˜](https://www.google.com/maps/search/{urllib.parse.quote(map_q)})")
        st.divider()
