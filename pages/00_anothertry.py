import streamlit as st
import pandas as pd
import os
import urllib.parse

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ ë°ì´í„° ë¶„ì„", layout="wide")

# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • (ìŠ¬ë˜ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ìœˆë„ìš° ê²½ë¡œ ì˜¤ë¥˜ ë°©ì§€)
CSV_PATH = "C:/Users/jslee/Downloads/restaurantinseoul.csv"

@st.cache_data
def load_and_process_data(path):
    # [ì²´í¬ 1] íŒŒì¼ì´ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(path):
        return "FILE_NOT_FOUND"

    # [ì²´í¬ 2] í•œê¸€ ì¸ì½”ë”© ì‹œë„ (ê³µê³µë°ì´í„°ëŠ” ë³´í†µ cp949ë‚˜ euc-krì…ë‹ˆë‹¤)
    encodings = ['cp949', 'utf-8-sig', 'euc-kr']
    
    for enc in encodings:
        try:
            # ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ë¥¼ ìœ„í•´ chunksize ì‚¬ìš©
            # í•„ìš”í•œ ì»¬ëŸ¼: 4ë²ˆì§¸(3), 9ë²ˆì§¸(8), 10ë²ˆì§¸(9)
            container = []
            reader = pd.read_csv(
                path, 
                usecols=[3, 8, 9], 
                chunksize=50000, 
                low_memory=False, 
                encoding=enc
            )
            
            for chunk in reader:
                # ì»¬ëŸ¼ëª… ê°•ì œ ì§€ì •
                chunk.columns = ['status', 'name', 'category']
                
                # [í•„í„°ë§] 'íì—…' ë¬¸êµ¬ê°€ ë“¤ì–´ê°„ í–‰ ì‚­ì œ
                # ê²°ì¸¡ì¹˜(NaN) ì œê±° í›„ ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€ í™•ì¸
                filtered_chunk = chunk[chunk['status'].fillna('').str.contains("ì˜ì—…|ì •ìƒ")].copy()
                filtered_chunk = filtered_chunk[~filtered_chunk['status'].fillna('').str.contains("íì—…")].copy()
                
                container.append(filtered_chunk)
            
            # ëª¨ë“  ì¡°ê° í•©ì¹˜ê¸°
            full_df = pd.concat(container, ignore_index=True)
            return full_df
            
        except (UnicodeDecodeError, ValueError):
            continue # ì¸ì½”ë”©ì´ë‚˜ ì»¬ëŸ¼ ì¸ë±ìŠ¤ê°€ ì•ˆ ë§ìœ¼ë©´ ë‹¤ìŒ ì‹œë„
        except Exception as e:
            return f"ERROR: {str(e)}"
            
    return "ENCODING_ERROR"

# --- ë©”ì¸ ì‹¤í–‰ë¶€ ---
st.title("ğŸ´ ì„œìš¸ì‹œ ë§›ì§‘ ì •ë³´ ì„œë¹„ìŠ¤")
st.info(f"ğŸ“ ëŒ€ìƒ íŒŒì¼: {CSV_PATH}")

# ë°ì´í„° ë¡œë”© ì‹œì‘
with st.spinner('ë°ì´í„°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.'):
    result = load_and_process_data(CSV_PATH)

# ì—ëŸ¬ ì²˜ë¦¬ ë° í™”ë©´ êµ¬ì„±
if isinstance(result, str):
    if result == "FILE_NOT_FOUND":
        st.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {CSV_PATH}")
    elif result == "ENCODING_ERROR":
        st.error("âŒ íŒŒì¼ ì½ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì¸ì½”ë”© í˜•ì‹ì´ ë§ì§€ ì•Šê±°ë‚˜ ì»¬ëŸ¼ êµ¬ì„±ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
    else:
        st.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result}")
else:
    df = result
    st.success(f"âœ… ì˜ì—… ì¤‘ì¸ ì‹ë‹¹ {len(df):,}ê°œë¥¼ ë¡œë”©í–ˆìŠµë‹ˆë‹¤.")

    # 10ë²ˆì§¸ ì»¬ëŸ¼(category) ê¸°ë°˜ LoV ìƒì„±
    # ê²°ì¸¡ì¹˜ ì œê±° í›„ ì •ë ¬
    categories = sorted(df['category'].dropna().unique().tolist())
    
    # ì‚¬ì´ë“œë°”ì—ì„œ ì¹´í…Œê³ ë¦¬ ì„ íƒ
    selected_category = st.selectbox("ğŸ¯ ìŒì‹ ì¢…ë¥˜(ì—…íƒœ)ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì „ì²´"] + categories)

    # í•„í„°ë§ ì ìš©
    if selected_category != "ì „ì²´":
        final_df = df[df['category'] == selected_category]
    else:
        final_df = df

    st.subheader(f"ğŸ“ '{selected_category}' ê²€ìƒ‰ ê²°ê³¼ (ìµœìƒìœ„ 20ê°œ)")

    # ê²°ê³¼ ì¶œë ¥
    top_20 = final_df.head(20)
    
    if len(top_20) > 0:
        for i, row in top_20.iterrows():
            # êµ¬ê¸€ ê²€ìƒ‰ URL ìƒì„± (ì‹ë‹¹ì´ë¦„ + ì—…íƒœ)
            search_query = urllib.parse.quote(f"ì„œìš¸ {row['name']} {row['category']}")
            google_url = f"https://www.google.com/search?q={search_query}"
            
            with st.container():
                col1, col2 = st.columns([4, 1])
                col1.write(f"**{i+1}. {row['name']}** \n({row['category']})")
                col2.markdown(f"[â­ êµ¬ê¸€ê²€ìƒ‰]({google_url})")
                st.divider()
    else:
        st.warning("ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
