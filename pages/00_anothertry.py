import streamlit as st
import os

# í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ í™”ë©´ì— í‘œì‹œ
st.write("í˜„ì¬ ì½”ë“œ ì €ì¥ ìœ„ì¹˜:", os.path.abspath(__file__))


import streamlit as st
import pandas as pd
import requests
import io
import urllib.parse

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ ê²€ìƒ‰", layout="wide")

# ë³´ë‚´ì£¼ì‹  íŒŒì¼ ID
GOOGLE_FILE_ID = '15qLFBk-cWaGgGxe2sPz_FdgeYpquhQa4'

@st.cache_data(show_spinner=False)
def load_large_csv_from_gdrive(file_id):
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë³´ì•ˆ ê²½ê³ ë¥¼ ìš°íšŒí•˜ì—¬ ëŒ€ìš©ëŸ‰ CSVë¥¼ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜"""
    download_url = "https://docs.google.com/uc?export=download"
    session = requests.Session()
    
    # 1ì°¨ ìš”ì²­: ë³´ì•ˆ í† í°(confirm) í™•ì¸
    response = session.get(download_url, params={'id': file_id}, stream=True)
    
    token = None
    for key, value in response.cookies.items():
        if key.startswith('download_warning'):
            token = value
            break
            
    # 2ì°¨ ìš”ì²­: í† í°ì´ ìˆë‹¤ë©´ í¬í•¨í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    if token:
        params = {'id': file_id, 'confirm': token}
        response = session.get(download_url, params=params, stream=True)
    
    # ì‘ë‹µ ë‚´ìš©ì´ HTML(ë¡œê·¸ì¸/ê¶Œí•œ í˜ì´ì§€)ì¸ì§€ ì²´í¬
    if "html" in response.headers.get('Content-Type', '').lower():
        return "AUTH_ERROR"

    # ì¸ì½”ë”© ìˆœì°¨ ì‹œë„ ë° ë°ì´í„° ì •ë¦¬
    try:
        content = response.content
        for enc in ['cp949', 'utf-8-sig', 'euc-kr']:
            try:
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ í•„ìš”í•œ 4ê°œ ì»¬ëŸ¼ë§Œ ë¡œë“œ
                df = pd.read_csv(
                    io.BytesIO(content),
                    encoding=enc,
                    usecols=[3, 8, 9, 18],
                    on_bad_lines='skip',
                    low_memory=False,
                    dtype=str
                )
                df.columns = ['status', 'name', 'category', 'address']
                
                # [ìš”êµ¬ì‚¬í•­] íì—… ë°ì´í„° ì‚­ì œ
                df = df[~df['status'].fillna('').str.contains("íì—…|ì·¨ì†Œ|ë§ì†Œ")].copy()
                return df
            except:
                continue
        return "PARSE_ERROR"
    except Exception as e:
        return f"ERROR: {str(e)}"

# --- ë©”ì¸ ì‹¤í–‰ë¶€ ---
st.title("ğŸ´ ì„œìš¸ì‹œ ë§›ì§‘ ê°€ì´ë“œ (TOP 20)")

with st.spinner('êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
    data = load_large_csv_from_gdrive(GOOGLE_FILE_ID)

if data == "AUTH_ERROR":
    st.error("âŒ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì ‘ê·¼ ê±°ë¶€ (AUTH_ERROR)")
    st.write("ê³µìœ  ì„¤ì •ì´ 'ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì' ì„ì—ë„ ì•ˆ ëœë‹¤ë©´, ì•„ë˜ ì£¼ì†Œë¥¼ ë¸Œë¼ìš°ì €ì— ì…ë ¥í–ˆì„ ë•Œ íŒŒì¼ì´ ë°”ë¡œ ë‹¤ìš´ë¡œë“œ ë˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    st.code(f"https://docs.google.com/uc?export=download&id={GOOGLE_FILE_ID}")
elif data == "PARSE_ERROR":
    st.error("âŒ ë°ì´í„° í•´ì„ ì‹¤íŒ¨ (ì¸ì½”ë”© ë¬¸ì œ)")
elif isinstance(data, pd.DataFrame):
    st.success(f"âœ… {len(data):,}ê°œì˜ ì‹ë‹¹ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    # ì—…ì¢… ì„ íƒ LoV
    category_list = sorted(data['category'].dropna().unique().tolist())
    selected = st.selectbox("ğŸ± ìŒì‹ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì „ì²´"] + category_list)

    filtered = data if selected == "ì „ì²´" else data[data['category'] == selected]

    # ê²°ê³¼ ì¶œë ¥
    for i, row in filtered.head(20).iterrows():
        search_q = urllib.parse.quote(f"ì„œìš¸ {row['name']} {row['category']} í‰ì  ë¦¬ë·°")
        map_q = urllib.parse.quote(f"{row['name']} {row['address']}")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"### {row['name']}")
            st.caption(f"ğŸ“‚ {row['category']} | ğŸ“ {row['address']}")
        with col2:
            st.write("")
            st.markdown(f"[â­ í‰ì í™•ì¸](https://www.google.com/search?q={search_q})")
            st.markdown(f"[ğŸ“ ì§€ë„ë³´ê¸°](https://www.google.com/maps/search/{map_q})")
        st.divider()
