import streamlit as st
import pandas as pd
import requests
import io
import urllib.parse

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ TOP 20", layout="wide")

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ íŒŒì¼ ID ë° ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL
GOOGLE_FILE_ID = '15qLFBk-cWaGgGxe2sPz_FdgeYpquhQa4'
DIRECT_URL = f'https://drive.google.com/uc?export=download&id={GOOGLE_FILE_ID}'

@st.cache_data(show_spinner=False)
def load_and_process_data(url):
    try:
        # í´ë¼ìš°ë“œì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # [í•µì‹¬] ë§Œì•½ ì‘ë‹µì´ HTMLì´ë¼ë©´(ê¶Œí•œ ë¬¸ì œ), ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        if "html" in response.headers.get('Content-Type', '').lower():
            return "êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê¶Œí•œ ì—ëŸ¬: íŒŒì¼ì´ ê³µê°œ ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤. 'ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì'ë¡œ ì„¤ì •ì„ ë³€ê²½í•´ì£¼ì„¸ìš”."

        # ì—¬ëŸ¬ ì¸ì½”ë”©ê³¼ êµ¬ë¶„ìë¥¼ ì‹œë„í•˜ëŠ” ë£¨í”„
        for enc in ['cp949', 'utf-8-sig', 'euc-kr']:
            try:
                # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜ í›„ ì½ê¸°
                data_stream = io.BytesIO(response.content)
                df = pd.read_csv(
                    data_stream, 
                    encoding=enc,
                    sep=None,          # êµ¬ë¶„ì ìë™ ê°ì§€ (ì½¤ë§ˆ, íƒ­ ë“±)
                    engine='python',   # ìë™ ê°ì§€ë¥¼ ìœ„í•´ python ì—”ì§„ ì‚¬ìš©
                    on_bad_lines='skip', 
                    dtype=str,         # ëª¨ë“  ë°ì´í„°ë¥¼ ì¼ë‹¨ ë¬¸ìì—´ë¡œ ì½ì–´ ì˜¤ë¥˜ ë°©ì§€
                    low_memory=False
                )
                
                # í•„ìš”í•œ ì»¬ëŸ¼ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼)
                # 4ë²ˆì§¸(3), 9ë²ˆì§¸(8), 10ë²ˆì§¸(9), 19ë²ˆì§¸(18)
                if df.shape[1] >= 19:
                    df_final = df.iloc[:, [3, 8, 9, 18]].copy()
                    df_final.columns = ['status', 'name', 'category', 'address']
                    
                    # íì—… ë°ì´í„° ì œê±° (í‚¤ì›Œë“œ í™•ì¥)
                    df_final = df_final[~df_final['status'].fillna('').str.contains("íì—…|ì·¨ì†Œ|ë§ì†Œ|ì •ë¦¬")].copy()
                    
                    if not df_final.empty:
                        return df_final
            except Exception:
                continue
                
        return "ë°ì´í„° êµ¬ì¡° í•´ì„ ì‹¤íŒ¨: íŒŒì¼ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"ì—°ê²° ì˜¤ë¥˜: {str(e)}"

# --- UI ë¶€ë¶„ ---
st.title("ğŸ´ ì„œìš¸ì‹œ ì‹¤ì‹œê°„ ë§›ì§‘ ì¶”ì²œ ê°€ì´ë“œ")

with st.spinner('ë°ì´í„°ë¥¼ ì •ë°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
    data = load_and_process_data(DIRECT_URL)

if isinstance(data, str):
    st.error(data)
    st.markdown("""
    ### ğŸš© í•´ê²° ë°©ë²•:
    1. **êµ¬ê¸€ ë“œë¼ì´ë¸Œ ê³µìœ  í™•ì¸**: íŒŒì¼ ìš°í´ë¦­ -> ê³µìœ  -> 'ì œí•œë¨'ì„ **'ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì'**ë¡œ ë³€ê²½í•˜ì…¨ë‚˜ìš”?
    2. **íŒŒì¼ í˜•ì‹ í™•ì¸**: íŒŒì¼ì´ `.csv` í™•ì¥ìê°€ ë§ëŠ”ì§€, í˜¹ì‹œ ì—‘ì…€(`.xlsx`) íŒŒì¼ì€ ì•„ë‹Œì§€ í™•ì¸í•´ì£¼ì„¸ìš”. (ì—‘ì…€ì´ë©´ ì½”ë“œë¥¼ ë°”ê¿”ì•¼ í•©ë‹ˆë‹¤.)
    """)
else:
    st.success(f"âœ… ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ì´ {len(data):,}ê°œ ì˜ì—… ì¤‘)")

    # ì—…ì¢… LoV ìƒì„±
    category_list = sorted(data['category'].dropna().unique().tolist())
    selected_category = st.selectbox("ğŸ± ìŒì‹ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì „ì²´"] + category_list)

    filtered_df = data if selected_category == "ì „ì²´" else data[data['category'] == selected_category]

    st.divider()
    st.subheader(f"ğŸ“ '{selected_category}' ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ TOP 20")

    # ìƒìœ„ 20ê°œ ì¶œë ¥
    for i, row in filtered_df.head(20).iterrows():
        # ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë§í¬
        search_q = f"ì„œìš¸ {row['name']} {row['category']} í‰ì  ë¦¬ë·°"
        map_q = f"{row['name']} {row['address']}"
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"### {row['name']}")
            st.write(f"ğŸ“‚ {row['category']} | ğŸ“ {row['address']}")
        with col2:
            st.markdown(f"[â­ í‰ì  í™•ì¸](https://www.google.com/search?q={urllib.parse.quote(search_q)})")
            st.markdown(f"[ğŸ“ ì§€ë„ ë³´ê¸°](https://www.google.com/maps/search/{urllib.parse.quote(map_q)})")
        st.divider()
