import streamlit as st
import pandas as pd
import requests
import io
import urllib.parse

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„œìš¸ ë§›ì§‘ TOP 20", layout="wide")

# ë³´ë‚´ì£¼ì‹  íŒŒì¼ ID
GOOGLE_FILE_ID = '15qLFBk-cWaGgGxe2sPz_FdgeYpquhQa4'

@st.cache_data(show_spinner=False)
def download_large_file(file_id):
    """êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ ëŒ€ìš©ëŸ‰ íŒŒì¼ ë³´ì•ˆ ê²½ê³ ë¥¼ ìš°íšŒí•˜ì—¬ ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    base_url = "https://docs.google.com/uc?export=download"
    session = requests.Session()
    
    # 1ë‹¨ê³„: íŒŒì¼ IDë¥¼ í†µí•´ ë³´ì•ˆ í† í°(confirm token) í™•ì¸ ìš”ì²­
    response = session.get(base_url, params={'id': file_id}, stream=True)
    
    def get_confirm_token(response):
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                return value
        return None

    token = get_confirm_token(response)
    
    # 2ë‹¨ê³„: í† í°ì´ ìˆë‹¤ë©´ í† í°ì„ í¬í•¨í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ìš”ì²­
    if token:
        params = {'id': file_id, 'confirm': token}
        response = session.get(base_url, params=params, stream=True)
    
    # ì‘ë‹µì´ ì—¬ì „íˆ HTML(ê¶Œí•œ/ë¡œê·¸ì¸ í˜ì´ì§€)ì¸ì§€ ìµœì¢… í™•ì¸
    if "html" in response.headers.get('Content-Type', '').lower():
        return "AUTH_ERROR"
        
    return response.content

@st.cache_data(show_spinner=False)
def process_data(content):
    """ë‹¤ìš´ë¡œë“œëœ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ íŒë‹¤ìŠ¤ë¡œ ë³€í™˜í•˜ê³  í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜"""
    if content == "AUTH_ERROR":
        return "AUTH_ERROR"
        
    # ì¸ì½”ë”© ìˆœì°¨ ì‹œë„ (CP949 -> UTF-8-SIG -> EUC-KR)
    for enc in ['cp949', 'utf-8-sig', 'euc-kr']:
        try:
            df = pd.read_csv(
                io.BytesIO(content),
                encoding=enc,
                usecols=[3, 8, 9, 18], # ìƒíƒœ, ì´ë¦„, ì—…ì¢…, ì£¼ì†Œ
                on_bad_lines='skip',
                low_memory=False,
                dtype=str
            )
            df.columns = ['status', 'name', 'category', 'address']
            
            # [ìš”êµ¬ì‚¬í•­] 'íì—…' ì œì™¸
            df = df[~df['status'].fillna('').str.contains("íì—…|ì·¨ì†Œ|ë§ì†Œ")].copy()
            return df
        except:
            continue
    return "PARSE_ERROR"

# --- ë©”ì¸ ì‹¤í–‰ë¶€ ---
st.title("ğŸ´ ì„œìš¸ì‹œ ì‹¤ì‹œê°„ ë§›ì§‘ ì¶”ì²œ ê°€ì´ë“œ")

with st.spinner('êµ¬ê¸€ í´ë¼ìš°ë“œì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë™ê¸°í™” ì¤‘ì…ë‹ˆë‹¤...'):
    raw_content = download_large_file(GOOGLE_FILE_ID)
    data = process_data(raw_content)

if data == "AUTH_ERROR":
    st.error("âŒ AUTH_ERROR: êµ¬ê¸€ ë“œë¼ì´ë¸Œê°€ ì ‘ê·¼ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤.")
    st.markdown("""
    **í•´ê²° ë°©ë²•:**
    1. êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ íŒŒì¼ ìš°í´ë¦­ -> **ê³µìœ ** -> **'ë§í¬ê°€ ìˆëŠ” ëª¨ë“  ì‚¬ìš©ì'**ë¡œ ë˜ì–´ìˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸!
    2. ì™„ë£Œ ë²„íŠ¼ì„ ëˆ„ë¥¸ í›„, ì´ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨(F5) í•´ì£¼ì„¸ìš”.
    """)
elif data == "PARSE_ERROR":
    st.error("âŒ PARSE_ERROR: ë°ì´í„° í˜•ì‹ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
elif isinstance(data, pd.DataFrame):
    st.success(f"âœ… {len(data):,}ê°œì˜ ì˜ì—… ì¤‘ì¸ ì‹ë‹¹ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    # ì—…ì¢… LoV
    category_list = sorted(data['category'].dropna().unique().tolist())
    selected = st.selectbox("ğŸ± ìŒì‹ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ì „ì²´"] + category_list)

    filtered = data if selected == "ì „ì²´" else data[data['category'] == selected]

    st.divider()
    st.subheader(f"ğŸ“ '{selected}' ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ TOP 20")

    # ìƒìœ„ 20ê°œ ì¶œë ¥
    for i, row in filtered.head(20).iterrows():
        search_q = urllib.parse.quote(f"ì„œìš¸ {row['name']} {row['category']} í‰ì  ë¦¬ë·°")
        map_q = urllib.parse.quote(f"{row['name']} {row['address']}")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"### {row['name']}")
            st.caption(f"ğŸ“‚ {row['category']} | ğŸ“ {row['address']}")
        with col2:
            st.write("")
            st.markdown(f"[â­ í‰ì í™•ì¸](https://www.google.com/search?q={search_q})")
            st.markdown(f"[ğŸ“ ì§€ë„ë³´ê¸°](https://www.google.com/maps/search/?api=1&query={map_q})")
        st.divider()
